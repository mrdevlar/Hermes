# Methodology
<!--
4. Methods
	- Reliability Analysis
	- Censoring
	- Accelerated Failure Time Model
	- Frailty & Hierarchical Model
	- Estimation (HMC and Gibbs)
	- Model Selection
-->

Time-to-event analysis/footnote{also known as survival analysis, reliability analysis and event history analysis} concerns itself with the modeling of the expected duration of time until an object experiences a well-defined event. This event is a transition between a finite number of possible states, such as from operational to non-operational or from alive to dead\cite{Andersen1992}. In this particular context, these states define the transition of a photovoltaic inverter from functional to failed.

To be able to model the time until these transitions, there needs to be precision in describing the phenomenon, and for that, mathematics is required. The following section describes the mathematical narrative that underly the construction, manipulation and estimation of Bayesian time-to-event models.

A description of the basic functions required to understand this class of models, their role in defining time-to-event distributions and their relationship to one another is provided. Then, censoring and truncation which are common characteristics of time-to-event data are examined. From these basics units, the accelerated failure time model, a linear model for time-to-event analysis is formulated. Following this,hierarchical model which describes shared risk, or frailty are introduced. Finally, model estimation with Markov Chain Monte Carlo using the statistical software Stan is demonstrated.



## Time-to-Event Functions

The purpose of this section is to firmly root time-to-event analysis in the broader context of statistical modeling. It demonstrates the mathematical functions required to interact with this class of models, as well as providing their relationships. As this section is foundational, its content can be attributed to a number of sources\cite{Aalen2008}\cite{Tableman2004}\cite{Klein2003}\cite{Kleinbaum2005}\cite{Cleves2008}.

Begin by defining $T$, a continuous non-negative random variable with an unknown distribution representing the time until a well-defined event. As it is a time, its support is constrained to all positive real numbers, ($T \ge 0, T \in \mathbb{R}$). Further, define $t$ to be the realization of this random variable at a specific point in time.

A familiar way of describing a probability distribution is to use probability density function (pdf) and cumulative distribution function (cdf). The pdf is the relative likelihood of a random variable taking a particular realization.

$$f(t) = \Pr(T = t)$$

The cdf defines the probability that a random variable will take on a value less than or equal to some realization. Thus, it defines a range of outcomes across a random variable.  

$$F(t) = \Pr(T \le t) = \int^t_0 f(x) dx $$

In the time-to-event context, the cdf of $T$ may not be particularly useful as the variables of interest that take on values greater than some realization. This is because in most cases, $T$ will be greater than the value observed, $t$. Fortunately, by definition, a random variable must sum to one and the complement of the cdf can be established through subtraction.

$$S(t) = P(T \ge t) = 1 - F(t) = \int^{\infty}_t f(x) dx$$

This returns the survival function\footnote{also denoted as the reliability function $R(t)$} which is the probability
that the well-defined event occurs after a specific point in time. It is considered the survival function because it provides the probability of surviving beyond an observed time $t$. Clearly, if the event occurs after a specific time, then the event has not yet occurred. As the event is death or failure, it is implied that the object of interest has survived up to that point.

As the survival function is the complement of the cdf, it inherits its properties. It is monotonically decreasing, with $S(0) = 1$ and $S(\infty) = \lim_{t\rightarrow \infty}S(t) = 0$. This formalizes the notion that when the event in question is failure, in the beginning all systems are operational, but given a long enough time frame all systems will eventually fail.

While the survival function focuses on the event not occurring, the hazard function focuses on the event occurring. The hazard function\footnote{also known as the conditional failure rate and force of mortality} is the instantaneous rate of failure given that failure has not yet occurred.

$$ h(t) = \lim_{\Delta t \rightarrow 0^+} \frac{P(t \le T < t + \Delta t | T \ge t)}{\Delta t} = \frac{f(t)}{S(t)} $$

It provides the potential of the event occurring within the next (limiting) time interval, assuming it has not yet taken place. It should be stressed, that this function does not return a probability, rather a rate ( $\frac{P}{\Delta t}$ ), and thus can take on values greater than one.

The hazard is far more useful from a practical perspective than the other constructions. The conditional formulation is especially important, as the hazard defines the risk only after excluding the prior occurrence of the event. This makes it a more natural expression of what is generally expressed as the risk of an event in time, which implicitly presupposes that the event has not yet occurred.

Numerically, the hazard is clearly linked to the future occurrence of the event. When the hazard is zero, the risk of the event occurring in the future is also zero. Conversely, when the hazard is infinite, the risk of the event occurring the in future is near certain.

The hazard is a limiting rate, and is concerned with the event within an instantaneous interval. Sometimes, it is beneficial to understand that potential up to an instance in time.

$$ H(t) = \int^t_0 h(u) du $$

This is the cumulative hazard function. It measures the total amount of risk that has been accumulated up to time $t$. The cumulative hazard can be understood as the number of times we would expect to observe the event in a given period of time, assuming the event were repeatable.

The pdf, cdf, survival, hazard and cumulative hazard functions all uniquely define the process generating the time-to-event data. As a result, it is possible to transform any of these functions into any other. This is useful because it provides insight into the relationships among the functions, but also because it allows for the use of the easiest constructions in modeling.

It is generally true that any pdf can be expressed as a derivative of its cdf. As a result, the survival function also provides a route back to the pdf. The negative derivative of the survival function returns the pdf. Such that the pdf can be redefined as follows:

$$ f(t) = \lim_{\Delta t \rightarrow 0^+} \frac{\Pr(t \le T < t + \Delta t) }{\Delta t} = \frac{d F(t)}{dt} = - \frac{d S(t)}{dt}$$

The hazard function and survival function are also intimately related to one another. 

From the above identity, it can be seen that the pdf in the definition of the hazard can be replaced with the negative derivative of the survival function. The same can be done in the definition of the hazard. It is then clear that the hazard can be expressed as the negative derivative of the logarithm of the survival function. 

$$ h(t) = \frac{f(t)}{S(t)} = - \frac{d S(t) / dt }{S(t)} = - \frac{d \ln \left( S(t) \right) }{dt} = - \frac{d}{dt} \ln S(t)$$

The above identity can also be used to express the cumulative hazard. Integrating from the starting time until a specific time, we find that the cumulative hazard can be defined as the negative logarithm of the survival function. 

$$ H(t) \int^t_0 \frac{f(u)}{S(u)}du = - \int^t_0 \frac{1}{S(u)} \left( \frac{d}{du} S(u)\right) du = -\ln S(t) $$

Whatever transformation is expressed in terms of logarithms, can be reversed through exponents. Thus the survival function can be defined as the exponent of the negative integral of the hazard function, or the negative cumulative hazard.

$$ S(t) = \exp \left (- \int_0^t h(u) du  \right ) = \exp(-H(t)) $$

The pdf can also be expressed in terms of the exponent of the negative integral of the hazard function, or the cumulative hazard.

$$ f(t) = h(t) \exp \left (- \int_0^t h(u) du  \right ) = h(t) \exp(-H(t)) $$

As can be seen, regardless which of the functions are available, it is possible to produce a transformation which will return all the others. 

<!-- 
Should I include conditional forms of the above, assuming that the onset of risk is not observed.

See: https://books.google.de/books?id=xttbn0a-QR8C&printsec=frontcover&hl=de#v=onepage&q&f=false
P.9
-->


## Truncation and Censoring

Truncation and censoring are defining elements of time-to-event models. 

The functions that have been described so far have been within the context of complete information. Thus, an environment where the time until an event is fully observed. However, this is unrealistic. In most real-world applications, it is impractical to wait until all systems in our population have failed. Therefore, time-to-event models often exist in incomplete information. Truncation and censoring provide mechanisms to formalize how that state of limited information affects the model. 






<!--
Proportional Hazards models seek to find a shift in the hazard rate for an individual at every age of *t*.


- Distribution Choice
  - Weibull
  - ?Lognormal
  - ?Loglogistic
  - Gompertz


- Stratification == Frailty
  - Traditional regression methods would presume a shared fixed effect for the shape or scale of the distribution.


- Considerations for how deal with data recording, what to record and why?
	- Failure autopsy, sensors or maintenance testing
	- Continuous monitoring (economically impractical)

- The type of estimation done on the baseline hazard has implications for future forecasting. Non-parametric models lack the ability to forecast future failure rates beyond the last failure time.

# Notes

 - "Whereas OLS produces a predicted dependent variable (fitted value) for each individual; Cox regression produces a *probability distribution for the duration of each individual's waiting time*"
 - "Whereas in OLS a change in the estimated parameter's value implies a linear change in every individual's fitted value; in Cox regression a change in an estimated parameter implies *a proportional shift in each individual's hazard rate* **at all ages**"
	- [Cite](http://courses.demog.berkeley.edu/mason213F15/)



The first feature that distinguishes time-to-event analysis from traditional statistical methods is a lack of normally distributed errors.\cite{Guo2010} Time is always positive quantity, this alone should invalidate the normal error model. However, more compelling is the notion that the rate of survival distribution does not necessarily have to have a single mode. For example, when examining the survival time of individuals following a difficult surgical procedure, one would expect to see two groups of deaths. One shortly after the procedure, possibly due to complications, and a second group of deaths long after if the reason for the surgery reoccurs. This type of bimodality is unable to be properly modeled using standard OLS regression.

The second distinguishing feature is incomplete observation, or censoring, which obscures the time until an event occurs. For example, when analyzing the time until failure of an industrial device, it is often impractical to wait until all the devices in the population have failed before drawing conclusions. Therefore, the data we utilize gives us only a partial view of the duration until the event occurs in the population. Censoring comes in various classes and types, depending on the segment of time is unobserved. Right censoring is the most common class, and refers to the situation where the event is observed only if it occurs prior to some pre-specified time.\cite{Klein2003} In our case, we only observe a photovoltaic inverter failure if it has occurred in the past. Our prespecified time, is this moment, and all functioning inverters are thus right censored.


 -->


<!--

- Basic building blocks
- Relationships among the basic building blocks
- Survival function == Reliability function
- Hazard function == risk, intensity, mortality rate or conditional failure rate
- Better Segways between functions
- Maybe add examples?

-->


<!--
 - Censoring
 -
 -->

<!--
Moar stuff (but does it need a separate chapter or integrate here? Probably integrate here.)

 - Censoring
 - PH and AFT Models
 -->


## Extended Reading

For additional treatments of the topic, several notable sources can be accessed. For a comprehensive mathematical treatment of classical survival analysis, the reader is directed to Klein and Moeschberger \cite{Klein2003}. For a mathematical treatment of survival analysis based on counting processes, the reader is directed to Andersen\cite{Andersen1992} and Aalen, Borgan and Gjessing\cite{Aalen2008}. If the reader is interested in a pedagogical viewpoint on survival analysis or model computation, the reader should explore Kleinbaum and Klein\cite{Kleinbaum2005} and Tableman and Kim\cite{Tableman2004}. For a treatment of Bayesian time-to-event analysis, Ibrahim, Chen and Sinha\cite{Ibrahim2005} provide a foundational work. 
