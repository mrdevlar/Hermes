# Analysis

<!-- Intro Here REWRITE ME! -->

This section outlines the mechanism for building a predictive maintenance process. It begins by restating the goal of predictive maintenance in precise terms, illustrating what is possible given the volume of data available. It then explores data management and feature engineering. The quality of a predictive maintenance process in the photovoltaic domain largely depends on the ability to extract meaningful covariates from the small amount of relevant information. How data should be combined and what methods can be used to improve the quality of covariates is discussed. Then, the data simulation process is returned to, this time with its mathematical underpinnigns fully exposed. A comprehensive structure of the simulated data set is provided. A brief introduction to Stan, the probabilistic programming language, is given. This includes a detailed set up of the time-to-event model described in the previous chapter. Finally, the model is fit, the evaluation of model performance is given.

## The Goal

The goal of a predictive maintenance process in the photovoltaic domain is to create a system that allows for the correct allocation of maintenance resources regarding inverters with the greatest potential for failure. Formally, this can be seen as creating an sequence of inverters ranked by hazard at a given time. Such that:

$$ h_{(1)}(t) > h_{(2)}(t) > \dots h_{(n)}(t) $$

Where the time point, $t$ is fixed at the current moment for prediction. As a hazard defines an instantaneous failure rate it is the logical choice for the central unit of modeling. It should be noted, the absolute value of the hazard is of secondary importance in this process. If maintenance resources are allocated as part of a continuous service it does not matter if the inverter with the highest hazard is likely to fail within the next day, week or month. It is still the most likely to fail and should still be the inverter to which the maintenance process is applied to first. Thus the exact amount of risk or the remaining usable life are secondary to the action they prompt. 

<!-- Possibly state alternatives here -->


## Feature Engineering

Feature engineering is sometimes considered the dark art of the statistical modeling process. Its informal nature makes it difficult to find a general consensus as to what the concept actually means within academic literature\cite{Zabokrtsky2015}. Loosely speaking, feature engineering is the art of creating covariates that conceptually embody aspects of a phenomena or object of observation. In this case, it refers to the task of finding and encoding historical data that can describe an inverter's health at any given moment. 

While a multitude of effort is expended generating novel  models which explain and predict better, considerably less attention is focused on what is inputs are useful in these models. No domain ever comes with all the ideal features built into existing data sets. Thus a degree of skill is required to extract and encode what is important. In a practical setting, like the development of a predictive maintenance process, the proper application of feature engineering is the difference between an effective or ineffective process\cite{Yu2010a}. Part of this limited attention stems from the fact that feature engineering is often domain-specific and cannot be adequately generalized. That said, there are themes that can be used to guide the process. The two primary sources of features are subject matter expertise and experimentation. Subject matter expertise can be further subdivided into model-specific and domain-specific insights.

An understanding and awareness of the details and limitations of the model is paramount to being able to create covariates that act as effective inputs. This requires subject matter expertise, which arises from knowledge of the functional attributes of the applied model, such as that presented in the previous chapter. Such knowledge can avoid the negative consequences of using data that may be suboptimal in combination with certain models. For example, when introducing maintenance logs as a time-dependent covariate, it is not sufficient to simply provide a binary variable that records whether an inspection occurred on a specific date. The functional form of the time-dependent model implies such a formulation is unlikely to serve as a sufficiently strong signal for the effect of regular inspection on lifetimes, as it lacks variability. However, the number of days since the last inspection is more likely to have a substantive effect. The functional structure of the model reveals these conditions.

Domain-specific subject matter expertise is an inescapable necessity for effective feature engineering. While there continues to be a drive toward greater automation in determining the relevance of covariates, human decision still remains paramount. In this area, domain-specific subject matter refers to an understanding of the contributing events that lead to failure in photovoltaic inverters. This includes knowledge about the various failure modes of an inverter and their origins, such as high voltage, extreme temperatures, water condensation, the lack of robust software by certain manufacturers and others factors. It also includes knowledge about the hardware itself and which components are under stress and contribute to failure, such as capacitors, semiconductor switches and housing materials\cite{Flicker2014}.

Experimentation is the last source of feature engineering and is meant to complement rather than substitute subject matter expertise. As its name suggests, this is where features are created and then tested to see whether or not they improve model performance. This includes experimenting with feature encodings, such as making use of power transformations as well as variable discretization, standardization and normalization. All of these changes can potentially improve model performance. It may also include generating new features based on suspicions of effects. For example, encoding days where with both very low temperature and humidity which can cause printed circuit boards to crack. Thus, suspicions about the nature of the system can be codified and tested to determine if they improve model performance. 


Feature engineering is more an art-form than a science. Despite the lack of a clear heuristic, there are some guidelines for working with time-to-event models. 

<!-- NEED MOAR ? -->

Time-to-event models have difficulty in dealing with noisy variables across small intervals. The purpose of these models is to determine the contribution of covariates to the probability of failure. However, the introduction of stochastic variables, such as daily temperature, may not have that desired effect. This is because the effect of temperature on an inverter is only relevant in certain extreme cases and not in any one case. The effect of a single day of extreme temperature is unlikely to result in a failure on that day. In effect, this violates the association that the model demands between covariate value and inverter status. This can be partially offset by the use of rolling aggregates, like average temperature over five days, that provide an average temperature over a specific interval. However, by definition, these extreme events are rare, and will likely be smoothed out in these rolling aggregates. Furthermore, this does not account for compounding effects. A better solution is the use of monotonically increasing variables like a count of extreme temperatures across a specific interval. Of course, such a feature requires a definition for 'extreme', one which should be subject to experimentation on a validation set. 


## Data Management and Simulation

Raw data can rarely be input into a model as-is. The first chapter broadly examined the standard data sources for a predictive maintenance process as well as provided a general outline of the simulated data. This section returns to these topics and examines the structure of the input data as well as how it is generated. 

Data required for a predictive maintenance process often comes from several different database tables meant to record different aspects of a collection of systems. This includes databases that document system attributes, maintenance and failure history and condition monitoring and usage patterns. Conversely, time-to-event models, require a form of attribute-value data. Attribute-value data can be roughly thought of as tabular data. Each row is a system, and each column encodes a covariate, with one column containing the failure status of the system. Time-to-event models, require a slight expansion of this construction, to account for time-dependent variables. First, an additional column that contains the identifier of system observed is needed. Second, two columns that define the interval across which the time-dependent observation is made is also required. 

<!-- Insert table with example -->

<!-- Probability interval transformation -->


Inputs should be structured as a tuple, $(Y_i, d_i, Z_j, \textbf{x}_{ij}, )$, such that $Y_i$ is the observed time, $d_i$ is the censoring indicator, $Z_j$ the category 


## Outline

- Data management and Feature Engineering
	- Doing a lot with a little
	- The data comes in parts but that is not the format that is required for the model
	- Here is how they can/should be collapsed into a workable data set that is the quadruple $(Y_i, d_i, X_i, Z_i)$
	- Averages across periods to diminish volatility, vs raw data
	- Counts of binary variables over time
	- Time dependent covariates are just interval censored
	- Signals Processing is out of scope
	- Testing the resulting covariates is more important than their logical construction, use what works!
		- Encode subject matter expertise into covariates, statistics is **not itself** the answer
- Data Generation Redux - Now with Numbers
	- N = 500, 5 Parks, uneven number of inverters at parks
	- $T = H_0^{-1}\left [\frac{-\ln(U)}{w_{ij} \exp(\boldsymbol\beta^T \textbf{x}_{ij})}  \right ] $
	- Weibull Version
	- $ T = \left [\frac{-\ln(U)}{w_{ij}\lambda \exp(\boldsymbol\beta^T \textbf{x}_{ij})}  \right ]^{1/\nu} $
	- Big Table
		- Define Failure Mode (Failure is requires replacement)
			- Infant Mortality
			- Old Age Mortality
		- Define each category $w_{ij}$
			- Park - Big $\sigma^2$
			- Inverter Type - Small $\sigma^2$
		- Define each covariate, $\textbf{x}_{ij}$
			- Days of operation
			- Standardized Electrical Output - $\mathcal{N}(0,1)$
			- Maintenance Logs
				- Count of repairs
				- Days since last repair
				- Count of replacement
				- Days since last replacement part
			- Park Level Data
				- Standardized Avg Temperature
				- Standardized Humidity
				- Interaction effect between low avg temp and low humidity
- Stan and a medium for model fitting
	- Introduce Stan
		- Successor to BUGS and JAGS
		- Give BRIEF introduction to the components
			- Data
			- Parameters
			- Model
			- Derived Quantities
			- Functions (Recent)
		- Model construction
			- Define hazard functions
			- Allow Stan to take care of the rest
- Predictive performance and model comparison
	- Ideal case
		- Model residual lifetime
		- Concordance between prediction and failure
		- It is highly like that it this is not the metric to evaluate the model by, due to censoring will always pose challenges
	- Compromise Case
		- Highest hazard in next instantaneous period
		- Some indication of the urgency of the first instance of failure
	- Evaluating Model Performance
		- Deviance-based methods, WAIC
		- Hazards of cross validation in sequence learning
			- Weaker cross validation that retains sequences. 
		- Unbalanced data issues (99% not failed)
- Fit Model
	- Present output

<!-- 
## Conclusion?

IF we know that certain factors contribute to the deterioration of objects in the environment, whether from literature or subject-matter experts in the company, we should make use of them. Simply inputing data from existing systems and expecting perfect prediction is naive and foolish. Regardless of the perpetual drive toward greater automation in model optimization, ultimately, it is people that determine what is input. A mixed model can help provide a degree of flexibility to offset what is not being input. However, it is not magic (even if it may seem that way), and its performance can be easily overcome by a more sensible strategy of inputting variables.  -->


## Extended Reading

For an excellent guide to feature engineering of time-dependent processes, the reader is directed to Microsoft Playbook by Uz\cite{Uz2016}