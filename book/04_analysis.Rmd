# Analysis

<!-- Intro Here REWRITE ME! -->

This section outlines the mechanism for building a predictive maintenance process. It begins by restating the goal of predictive maintenance in precise terms, illustrating what is possible given the volume of data available. It then explores data management and feature engineering. The quality of a predictive maintenance process in the photovoltaic domain largely depends on the ability to extract meaningful covariates from the small amount of relevant information. How data should be combined and what methods can be used to improve the quality of covariates is discussed. Then, the data simulation process is returned to, this time with its mathematical underpinnigns fully exposed. A comprehensive structure of the simulated data set is provided. A brief introduction to Stan, the probabilistic programming language, is given. This includes a detailed set up of the time-to-event model described in the previous chapter. Finally, the model is fit, the evaluation of model performance is given.

## The Goal

The goal of a predictive maintenance process in the photovoltaic domain is to create a system that allows for the correct allocation of maintenance resources regarding inverters with the greatest potential for failure. Formally, this can be seen as creating an sequence of inverters ranked by hazard at a given time. Such that:

$$ h_{(1)}(t) > h_{(2)}(t) > \dots h_{(n)}(t) $$

Where the time point, $t$ is fixed at the current moment for prediction. As a hazard defines an instantaneous failure rate it is the logical choice for the central unit of modeling. It should be noted, the absolute value of the hazard is of secondary importance in this process. If maintenance resources are allocated as part of a continuous service it does not matter if the inverter with the highest hazard is likely to fail within the next day, week or month. It is still the most likely to fail and should still be the inverter to which the maintenance process is applied to first. Thus the exact amount of risk or the remaining usable life are secondary to the action they prompt. 

<!-- Possibly state alternatives here -->


## Feature Engineering

Feature engineering is sometimes considered the dark art of the statistical modeling process. Its informal nature makes it difficult to find a general consensus as to what the concept actually means within academic literature\cite{Zabokrtsky2015}. Loosely speaking, feature engineering is the art of creating covariates that conceptually embody aspects of a phenomena or object of observation. In this case, it refers to the task of finding and encoding historical information that can describe an inverter's health at any given moment. 

While a multitude of effort is expended generating novel  models which explain and predict better, considerably less attention is focused on what is inputs are useful in these models. No domain ever comes with all the ideal features built into existing data sets. Thus a degree of skill is required to extract and encode what is important. In a practical setting, like the development of a predictive maintenance process, the proper application of feature engineering is the difference between an effective or ineffective process\cite{Yu2010a}. Part of this limited attention stems from the fact that feature engineering is often domain-specific and cannot be adequately generalized. That said, there are themes that can be used to guide the process. The two primary sources of features are subject matter expertise and experimentation. Subject matter expertise can be further subdivided into model-specific and domain-specific insights.

An understanding and awareness of the details and limitations of the model is paramount to being able to create covariates that act as effective inputs. This requires subject matter expertise, which arises from knowledge of the functional attributes of the applied model, such as that presented in the previous chapter. Such knowledge can avoid the negative consequences of using data that may be suboptimal in combination with certain models. For example, when introducing maintenance logs as a time-dependent covariate, it is not sufficient to simply provide a binary variable that records whether an inspection occurred on a specific date. The functional form of the time-dependent model implies such a formulation is unlikely to serve as a sufficiently strong signal for the effect of regular inspection on lifetimes, as it lacks variability. However, the number of days since the last inspection is more likely to have a substantive effect. The functional structure of the model reveals these conditions.

Domain-specific subject matter expertise is an inescapable necessity for effective feature engineering. While there continues to be a drive toward greater automation in determining the relevance of covariates, human decision still remains paramount. In this area, domain-specific subject matter refers to an understanding of the contributing events that lead to failure in photovoltaic inverters. This includes knowledge about the various failure modes of an inverter and their origins, such as high voltage, extreme temperatures, water condensation, the lack of robust software by certain manufacturers and others factors. It also includes knowledge about the hardware itself and which components are under stress and contribute to failure, such as capacitors, semiconductor switches and housing materials\cite{Flicker2014}.

Experimentation is the last source of feature engineering and is meant to complement rather than substitute subject matter expertise. As its name suggests, this is where features are created and then tested to see whether or not they improve model performance. This includes experimenting with feature encodings, such as making use of power transformations as well as variable discretization, standardization and normalization. All of these changes can potentially improve model performance. It may also include generating new features based on suspicions of effects. For example, encoding days where with both very low temperature and humidity which can cause printed circuit boards to crack. Thus, suspicions about the nature of the system can be codified and tested to determine if they improve model performance. 


Feature engineering is more an art-form than a science. Despite the lack of a clear heuristic, there are some guidelines for working with time-to-event models. 

<!-- NEED ONE MOAR ? -->

Time-to-event models have difficulty in dealing with noisy variables across small intervals. The purpose of these models is to determine the contribution of covariates to the probability of failure. However, the introduction of stochastic variables, such as daily temperature, may not have that desired effect. This is because the effect of temperature on an inverter is only relevant in certain extreme cases and not in any one case. The effect of a single day of extreme temperature is unlikely to result in a failure on that day. In effect, this violates the association that the model demands between covariate value and inverter status. This can be partially offset by the use of rolling aggregates, like average temperature over five days, that provide an average temperature over a specific interval. However, by definition, these extreme events are rare, and will likely be smoothed out in these rolling aggregates. Furthermore, this does not account for compounding effects. A better solution is the use of monotonically increasing variables like a count of extreme temperatures across a specific interval. Of course, such a feature requires a definition for 'extreme', one which should be subject to experimentation on a validation set. 


## Data Management and Simulation

Raw data can rarely be input into a model as-is. The first chapter broadly examined the standard data sources for a predictive maintenance process as well as provided a general outline of the simulated data. This section returns to these topics and examines the structure of the input data. It then turns its attention to the process of data simulation, introducing the quantile function and the inverse cumulative hazard. Finally, it presents the data used in the subsequent analysis.

Data required for a predictive maintenance process often comes from several different database tables meant to record different aspects of a collection of systems. This includes databases that document system attributes, maintenance and failure history and condition monitoring and usage patterns. Conversely, time-to-event models, require a form of attribute-value data. Attribute-value data can be roughly thought of as tabular data. Each row is a system, and each column encodes a covariate, with one column containing the failure status of the system. Time-to-event models, require an expansion of this construction, to account for time-dependent variables. First, an additional column that contains the identifier of system observed is needed. Second, two columns that define the interval across which the time-dependent observation is made is also required. 

$$ (Y_{i(t_0)}, Y_{i(t_1)}, Z_j, W_k,  \textbf{x}_{ijk}) $$
<!-- Down Arrow -->
<!-- Insert table with example -->

The above table gives an illustration of how data should be structured on input to a time-to-event model. As shown above, the rows no longer represent independent observations. A collection of rows can belong to a single system. Observations are now interval censored between time points $t_0$ and $t_1$. In the case of the photovoltaic data the interval granularity is days.

<!-- Inputs should be structured as a tuple, $(Y_i, d_i, Z_j, \textbf{x}_{ij}, )$, such that $Y_i$ is the observed time, $d_i$ is the censoring indicator, $Z_j$ the category  -->

<!-- ## Simulation Process -->

Data is simulated using inverse transform sampling. This method generates random values from any distribution using only uniform random values as inputs. Generally speaking, this enables the generation of values from any continuous distribution. 

Briefly, let $F$ be a continuous cdf. This guarantees that $F^{-1}$ exists as a function from $[0,1]$ to $\mathbb{R}$. If $U$ is defined as a uniform random variable on the unit interval, $[0,1]$ and $T = F^{-1}(U)$, then $T$ is a random variable with the cdf, $F(\cdot)$. As the function of a random variable is also a random variable itself, the inverse of the cdf, $F^{-1}(U)$, known as the quantile function, is also a random variable\cite{Blitzstein2014}. This implies that, $F^{-1}(U) = T$ if and only if $U = F(T)$. Thus, all that is required to generate random values from any given distribution is the quantile function and uniform random variables. 

In this case, there is a desire to generate survival lifetimes. In the previous chapter, the cdf of a lifetime distribution was given as:

$$ F(t) = 1 - S(t) = 1 - \exp(-H(t)) $$

Due to symmetry, a uniform random variable is defined on the unit interval for the survival function as well as the cdf. If $U \sim \text{Unif}[0,1]$ then $(U-1) \sim \text{Unif}[0,1]$ as well. Thus:

$$ U = \exp(-H(T)) \sim \text{Unif}[0,1] $$

As long as all hazards are defined to be strictly positive, $h(t) > 0$, the cumulative hazard, $H(t)$ is invertible and the survival time can be expressed as:

$$ T = H^{-1}(-\ln(U)) $$

Fortunately, for all of the models presented thus far an inverse of the cumulative hazard exists\footenote{except for the Gamma function}. From this starting point it is possible to generate models with considerably more complex features.

For the Multiplicative Hazards Model with covariates, survival times are generated the following manner\cite{Bender2005}:

$$ T = H_0^{-1}(-\ln(U) \exp(-\boldsymbol\beta^T \textbf{x})) $$

Where $H_0^{-1}$ is the inverse of the baseline cumulative hazard function which is multiplied by the exponentiated effect of the covariates. 

This simulation can be extended further to include the a Frailty term, $Z_j$, it then becomes\cite{Romdhane2015}:

$$ T = H_0^{-1}\left [\frac{-\ln(U)}{Z_{j} \exp(\boldsymbol\beta^T \textbf{x}_{ij})}  \right ] $$

Or a time-dependent covariate:

$$ T = \left [\frac{1}{k} \ln\left ( \frac{(1 + \nu)(-\ln(u))}{\beta_t \lambda \nu \exp(\boldsymbol\beta^T \textbf{x}) } \right )  \right ]^{\frac{1}{(1+\nu)}} $$

Where, the the time-dependent covariate, $k$, is continuous and generated by a function that has a multiplicative effect on time, $z(k) = kt$ \cite{Austin2012}. 

Of course, the precise model depends on the parameterization of the baseline cumulative hazard. As stated in the previous chapter, numerous different distributions can be used for modeling lifetimes and can therefore be used for this baseline cumulative hazard. 

For the simulated data, a Weibull baseline hazard is employed. The resulting model with shared Frailties can therefore be simulated as follows:

$$T = \left [\frac{- \ln(u) }{Z_{j} \lambda \exp(\boldsymbol\beta^T \textbf{x}_{ij})}  \right ]^{1/\nu} $$

Where $u$ is the realization of a uniformly distributed random variable, $U \sim \text{Unif}[0,1]$ and $\nu$ and $\lambda$ are, respectively, the shape and scale parameters of Weibull distribution.

<!-- 
Maybe include an explicit extension about simulating time-dependent covariates 
-->



<!-- 
- Data Generation Redux - Now with Numbers
	- N = 500, 5 Parks, uneven number of inverters at parks
	- Big Table
		- Define Failure Mode (Failure is requires replacement)
			- Infant Mortality
			- Old Age Mortality
		- Define each category $w_{ij}$
			- Park - Big $\sigma^2$
			- Inverter Type - Small $\sigma^2$
		- Define each covariate, $\textbf{x}_{ij}$
			- Days of operation
			- Standardized Electrical Output - $\mathcal{N}(0,1)$
			- Maintenance Logs
				- Count of repairs
				- Days since last repair
				- Count of replacement
				- Days since last replacement part
			- Park Level Data
				- Standardized Avg Temperature
				- Standardized Humidity
				- Interaction effect between low avg temp and low humidity
 -->



## Stan

<!-- \footnote{The technical details of the fitting process that Stan uses were discussed in the previous chapter.} -->

Stan is the most recent addition to the toolkit for fitting Bayesian models. Like its predecessors BUGS and JAGS, Stan seeks to abstract the task of fitting complex models with a general programmatic process. Stan makes use of Hamiltonian Monte Carlo for the sampling of continuous parameters from a model. The method was described briefly in the previous chapter. 

The reason for the existence of Stan stems from the performance difficulties of its predecessors in dealing with multilevel generalized linear modeling contexts. Requirements such as conjugate priors and a log-concave posterior density make certain models incredibly inefficient when Gibbs sampling is used. In the current context, the introduction of complex shared Frailties can create problems with these existing modeling tools. 

A Stan program defines a statistical model through a conditional probability function, $\Pr(\theta|y,x)$, where $\theta$ is a sequence of unknown modeled values while $y$ is composed of modeled known values and $x$ of unmodeled covariates and constants\cite{StanDevelopmentTeam2016}. 

Stan is an imperative probabilistic programming language. This implies that the language requires the user to tell it how to do something more than just declare that it should be done. Precise instructions have to be written verbatim to achieve the desired modeling outcome. Yet, as the previous chapter demonstrated, models are constructed by combining relatively simple building blocks. This permits models of any complexity to be defined and built upon.

```
functions {
	// ... function declarations and definitions ...
}
data {
	// ... declarations ...
}
transformed data {
	// ... declarations ... statements ...
}
parameters {
	// ... declarations ...
}
transformed parameters {
	// ... declarations ... statements ...
}
model {
	// ... declarations ... statements ...
}
generated quantities {
	// ... declarations ... statements ...
}
```
<!-- Figure, programming blocks of Stan program -->

A Stan program consists of variable declarations and statements, divided into a series of blocks written in a C-like syntax. This compartmentalizes operations, as variables can be declared in the block within which they are used. Generally, three blocks are used to define a statistical model. These are the data, parameters and model blocks. Figure XX demonstrates all the available blocks and their required order.

The data block declares the data required for the model. The parameter block declares the model parameters, or the unobserved random variables being sampled by the model. The model block defines the log probability function used to fit the model. 

The data and parameter blocks both handle declaration of variables. In Stan, random variables are handled differently depending on whether or not they are observed. Observed random variables are declared as data while unobserved random variables are declared as parameters. Unobserved random variables can be sampled from or inserted in subsequent blocks. This is especially useful in the computation of the log probability function. 

To facilitate processing two transformation blocks and generated quantities block are available. The transformation blocks allow for data and parameters to be altered and saved in the process of executing a Stan program. This provides additional flexibility in model building. The generated quantities block allows for the creation of values of importance, such as summary statistics. The block is run at the end of each sampling step, this permits for the tracking of intermediary values as the model is fit. 

Once a Stan program is defined, it is compiled into C++ code before being executed. The program begins by validating the known values of $y$ and $x$ and checking their types and constraints. It then generates a sequence of non-independent identically distributed parameter values, which have a marginal distribution of $\Pr(\theta|y,x)$. This translation into C++ code greatly improves the speed and portability of Stan-built models. However, it also results in requirements that do not arise in earlier Bayesian samplers. Notably, Stan is statically typed and requires that all variables have their constraints explicitly defined, if they exist. This is partially to enforce explicitness among model designers, but it is also to prevent modeling errors from creating problematic outputs. For example, a Weibull baseline hazard requires that both the shape and scale parameters be strictly positive otherwise the function is not defined. There is no reason for any sampler to make use of negative parameter values when sampling from these distributions. Strict type setting with constraints ensures that these types of errors are caught as soon as they occur rather than potentially disturbing final results. 

Once the sampler runs until convergence, Stan writes the output values of each parameter to disk in a csv file. This permits sampling to be done multiple different times with the values of each iteration updating the output. This is particularly useful for large models that may require a greater amount of computer time for each iteration.

Stan comes with a great many of built-in operations and functions. The basic operations are the same as one expects from any programming language, logical and arithmetic operations, matrix and array manipulation, type conversions as well as built in functions for handling mathematical operations like solving ordinary differential equations. The language also includes functions that encode most statistical distributions including those that are found in the previous chapter. Furthermore, sampling statements are used that vectorize the sampling from known distributions improving the speed of execution. 

While these built-ins are useful for the vast majority of tasks, time-to-event models pose a problem for vanilla Stan. Much of the difficulty in building time-to-event models in Stan stems from the presence of censoring and truncation in the data. While Stan does have some support for dealing with censoring and truncation, the nature of this support is fairly inflexible, especially when dealing with more complex models, such as those with time-dependent variables or Frailties. The recent introduction a function block in the language has largely enabled the fitting of time-to-event models in Stan. As the name suggests, the function block allows for any arbitrary function to be declared. This includes the ability to explicitly define a likelihood function.

<!-- Any parameters in the model must have a non-zero posterior probability or else the sampler will halt.  -->





## Outline

- Stan and a medium for model fitting
	- Introduce Stan
		- Successor to BUGS and JAGS
		- Give BRIEF introduction to the components
			- Data
			- Parameters
			- Model
			- Derived Quantities
			- Functions (Recent)
		- Model construction
			- Define hazard functions
			- Allow Stan to take care of the rest
- Predictive performance and model comparison
	- Ideal case
		- Model residual lifetime
		- Concordance between prediction and failure
		- It is highly like that it this is not the metric to evaluate the model by, due to censoring will always pose challenges
	- Compromise Case
		- Highest hazard in next instantaneous period
		- Some indication of the urgency of the first instance of failure
	- Evaluating Model Performance
		- Deviance-based methods, WAIC
		- Hazards of cross validation in sequence learning
			- Weaker cross validation that retains sequences. 
		- Unbalanced data issues (99% not failed)
- Fit Model
	- Present output

<!-- 
## Conclusion?

IF we know that certain factors contribute to the deterioration of objects in the environment, whether from literature or subject-matter experts in the company, we should make use of them. Simply inputing data from existing systems and expecting perfect prediction is naive and foolish. Regardless of the perpetual drive toward greater automation in model optimization, ultimately, it is people that determine what is input. A mixed model can help provide a degree of flexibility to offset what is not being input. However, it is not magic (even if it may seem that way), and its performance can be easily overcome by a more sensible strategy of inputting variables.  -->


## Extended Reading

For an excellent guide to feature engineering of time-dependent processes, the reader is directed to Microsoft Playbook by Uz\cite{Uz2016}. Inverse transform sampling in time-to-event models is covered by Bender, Augustin and Blettner\cite{Bender2005}, is extended to Frailties by Romdhane and Belkacem\cite{Romdhane2015} and into time-dependent covariates by Austin\cite{Austin2012} and Hendry\cite{Hendry2014}. For more information on the Stan probabilistic programming language, the Stan development team has a exhaustive manual in place\cite{StanDevelopmentTeam2016}.