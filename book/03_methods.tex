\chapter*{Methodology}
\markboth{Methodology}{Methodology}
\addcontentsline{toc}{chapter}{Methodology}

Time-to-event analysis/footnote{also known as survival analysis, reliability analysis and event history analysis} concerns itself with the modeling of the expected duration of time until an object experiences a well-defined event. This event is a transition between a finite number of possible states, such as from operational to non-operational or from alive to dead\cite{Andersen1992}. In this particular context, these states define the transition of a photovoltaic inverter from functional to failed.

To be able to model the time until these transitions, precision in describing phenomenon is needed, and for that, mathematics is required. The following chapter describes the mathematical narrative that underlies the construction, manipulation and estimation of Bayesian time-to-event models. It begins with the basic functions required to understand this class of models, their relationship, and role in defining time-to-event distributions. Then, censoring and truncation which are common characteristics of time-to-event data are examined. From these basics units, the Multiplicative Hazard Model for time-to-event analysis is formulated. Following this, a hierarchical model which describes shared risk, or Frailty is introduced. Finally, a description of the likelihood and model estimation with Hamiltonian Monte Carlo is given.



\section*{Time-to-Event Functions}
\addcontentsline{toc}{section}{Time-to-Event Functions}

The purpose of this section is to firmly root time-to-event analysis in the broader context of statistical modeling. It demonstrates the mathematical functions required to interact with this class of models, as well as providing their relationships. As this section is foundational, its content can be attributed to a diverse number of sources\cite{Aalen2008}\cite{Tableman2004}\cite{Klein2003}\cite{Kleinbaum2005}\cite{Cleves2008}\cite{Rodriguez2007}.

Begin, by defining $T$, a continuous non-negative random variable with an unknown distribution representing the time until a well-defined event. As it is a time, its support is constrained to all positive real numbers, ($T \ge 0, T \in \mathbb{R}$). Further, define $t$, to be the realization of this random variable at a specific point in time.

A familiar way of describing a probability distribution is to use probability density function (pdf) and cumulative distribution function (cdf). The pdf is the relative likelihood of a random variable taking a particular realization.

$$f(t) = \Pr(T = t)$$

The cdf defines the probability that a random variable will take on a value less than or equal to some realization. Thus, it defines a range of outcomes across the random variable.  

$$F(t) = \Pr(T \le t) = \int^t_0 f(x) dx $$

In the time-to-event context, the cdf of $T$ may not be particularly useful as the variables of interest usually take on values greater than some realization. This is because in most cases, $T$ will be greater than the value observed, $t$. Fortunately, by definition, a random variable must sum to one and the complement of the cdf can be established through subtraction.

$$S(t) = P(T \ge t) = 1 - F(t) = \int^{\infty}_t f(x) dx$$

This returns the survival function\footnote{also denoted as the reliability function $R(t)$} which is the probability
that the well-defined event occurs after a specific point in time. It is considered the survival function because it provides the probability of surviving beyond an observed time, $t$. Clearly, if the event occurs after a specific time, then the event has not yet occurred. As the event is death or failure, it is implied that the object of interest has survived up to that point.

As the survival function is the complement of the cdf, it inherits its properties. It is monotonically decreasing, with $S(0) = 1$ and $S(\infty) = \lim_{t\rightarrow \infty}S(t) = 0$. This formalizes the notion that when the event in question is failure, in the beginning all systems are operational, but given a long enough time frame all systems will eventually fail.

While the survival function focuses on the event not occurring, the hazard function focuses on the event occurring. The hazard function\footnote{also known as the conditional failure rate, intensity and force of mortality} is the instantaneous rate of failure given that failure has not yet occurred.

$$ h(t) = \lim_{\Delta t \rightarrow 0^+} \frac{P(t \le T < t + \Delta t | T \ge t)}{\Delta t} = \frac{f(t)}{S(t)} $$

It provides the potential of the event occurring within the next limiting time interval, assuming it has not taken place until now. It should be stressed, that this function does not return a probability, rather a rate ( $\frac{P}{\Delta t}$ ). It must be non-negative, such that $h(t) \ge 0$, and can take on values greater than one $[0, \infty)$.

The hazard is far more useful from a practical perspective than the other constructions. The conditional formulation is especially important, as the hazard defines the risk only after excluding the prior occurrence of the event. This makes it a more natural expression of what is generally asserted as the risk of an event in time, which implicitly presupposes that the event has not yet occurred.

Numerically, the hazard is clearly linked to the future occurrence of the event. When the hazard is zero, the risk of the event occurring in the next moment is also zero. Conversely, when the hazard is infinite, the risk of the event occurring the next instance is near certain.

The hazard is a limiting rate, and is concerned with the event within an instantaneous interval. At times, it is beneficial to understand that potential across an interval of time.

$$ H(t) = \int^t_0 h(u) du $$

This is the cumulative hazard function. It measures the total amount of risk that has been accumulated up to time $t$. The cumulative hazard can be understood as the number of times we would expect to observe the event in a given period of time, assuming the event were repeatable.

The relationship between the hazard and cumulative hazard is especially important in providing intuition. The hazard is a rate is defined in $\frac{1}{t}$ units. Whereas the cumulative hazard sums across those $\frac{1}{t}$ units. For example, if a rate of a particular event was ten, and five units of time passed, then fifty events would be expected within those five units. It is important to note, that this logic is one-directional and is based on an assumption of a constant hazard rate over the five units of time. There is nothing that makes this generally true. Thus, when starting with a cumulative hazard of fifty, it is possible that the hazard rate is fifteen, five, ten, five, and fifteen, respectively, for each of the five units of time. 

The pdf, cdf, survival, hazard and cumulative hazard functions all uniquely define the process generating time-to-event data. As a result, it is possible to transform any of these functions into any other. This is useful because it provides insight into the relationships among the functions but also because it allows for the use of the easiest constructions in modeling.

It is generally true that any pdf can be expressed as a derivative of its cdf. As a result, the survival function also provides a route back to the pdf. The negative derivative of the survival function returns the pdf. Such that the pdf can be redefined as follows:

$$ f(t) = \lim_{\Delta t \rightarrow 0^+} \frac{\Pr(t \le T < t + \Delta t) }{\Delta t} = \frac{d F(t)}{dt} = - \frac{d S(t)}{dt}$$

The hazard function and survival function are also intimately related to one another. From the above identity, it can be seen that the pdf in the definition of the hazard can be replaced with the negative derivative of the survival function. The same can be done in the definition of the hazard. It is then clear that the hazard can be expressed as the negative derivative of the logarithm of the survival function.

$$ h(t) = \frac{f(t)}{S(t)} = - \frac{d S(t) / dt }{S(t)} = - \frac{d \ln \left( S(t) \right) }{dt} = - \frac{d}{dt} \ln S(t)$$

The above identity can also be used to express the cumulative hazard. Integrating from a starting time until a specific time, demonstrates that the cumulative hazard can be defined as the negative logarithm of the survival function.

$$ H(t) = \int^t_0 \frac{f(u)}{S(u)}du = - \int^t_0 \frac{1}{S(u)} \left( \frac{d}{du} S(u)\right) du = -\ln S(t) $$

Whatever transformation is expressed in terms of logarithms, can be reversed through exponents. Thus, the survival function can be defined as the exponent of the negative integral of the hazard function, or the negative cumulative hazard.

$$ S(t) = \exp \left (- \int_0^t h(u) du  \right ) = \exp(-H(t)) $$

This allows for the conversion of a cumulative hazard back into a probability. Given the earlier example of a cumulative hazard of fifty the survival probability can be determined: $S(t) = \exp(-50) = 0.19 \cdot 10^{-22}$ which is a near zero probability of survival.

The pdf can also be expressed in terms of the exponent of the negative integral of the hazard function, or the cumulative hazard.

$$ f(t) = h(t) \exp \left (- \int_0^t h(u) du  \right ) = h(t) \exp(-H(t)) $$

As can be seen, regardless which of the functions are available, it is possible to produce a transformation which will return all the others. This is useful because generally, the hazard or survival time rather than the density are the objects of interest.

All of these functions can be conditioned to only extend to events that occur after a particular point in time. This is useful when dealing with lifetimes where the system has not been observed from the time it became operational. It is also useful when constructing expressions about particular intervals of time\cite{Cleves2008}.

$$ h(t|T > t_0) = h(t) $$

$$ H(t|T > t_0) = H(t) - H(t_0) $$

$$ F(t|T > t_0) = \frac{F(t) - F(t_0)}{S(t_0)} $$

$$ f(t|T > t_0) = \frac{f(t)}{S(t_0)} $$

$$ S(t|T > t_0) = \frac{S(t)}{S(t_0)} $$

One final construction of interest is the expected residual lifetime. This is the amount of life a particular system is expected to have given it has survived up to a specific time ($t_0$).

$$ E(t|T > t_0) = \int^\infty_{t_0} S(t) dt \cdot \frac{1}{S(t_0)} $$


\section*{Common Lifetime Distributions}
\addcontentsline{toc}{section}{Common Lifetime Distributions}


Any continuous distribution defined over the positive numbers can be used as a lifetime distribution. This section briefly covers several commonly used distributions for modeling lifetimes and provides justifications for their use. The exponential, Weibull, Gompertz and Gamma are described.

The simplest lifetime distribution can be defined by assuming a constant hazard rate ($\lambda$) over time.

\begin{align*}
h(t) = \lambda \tag{ $\lambda > 0$}
\end{align*}

As was demonstrated earlier, this can be transformed into a survival function by exponentiating the integral of all values from start until $t$. As the hazard is constant, this will simply be the exponent of the constant multiplied by the number of intervals present.

$$ S(t) = \exp \left (- \int_0^t \lambda\; du  \right ) = \exp (- \lambda\; t )$$

If transformed into a density, its characterization becomes immediately apparent.

$$ f(t) = - \frac{d S(t)}{dt} = \lambda\; \exp(- \lambda\; t) $$

This is the functional form of exponential distribution. It is useful in two areas in time-to-event analysis. First, it can be used to model events whose likelihood of occurrence does not vary with time. It can model systems that are not affected by wear or aging. For example, the exponential distribution is a suitable model for the release of particles from radioactive material\cite{Jowett1958}. However, these types of events are relatively rare in industrial settings. There are very few systems that do not experience some changes as a result of their length of operation. The second area of use for the exponential distribution is the discretizing of time. Time is continuous but data is not. Thus, there is a need to define how continuous time behaves across recorded intervals. The most consistently used assumption is that the hazard does not change within any given interval. If these intervals are sufficiently small such an assumption is perfectly valid.

Most lifetime distributions arise as generalizations of the exponential. They provide a more elaborate construction for the hazard over time based on some underlying logic.

The Gompertz distribution is a generalization of the exponential that introduces an exponential effect in the hazard over time.

\begin{align*}
h(t) = \lambda \exp(\varphi t) \tag{ $\lambda > 0, \varphi \in (-\infty, \infty) $}
\end{align*}

The formula was originally derived to characterize the exponential rise in death rates in humans between sexual maturity and old age\cite{Wienke2010}. It introduces a shape parameter $\varphi$ which controls the rate of change of the hazard over time. When $\varphi < 0$ the hazard declines over time. This can be used to model component "burn-in", which is the time shortly after beginning operations where mechanical parts often experience early failures. Conversely, when $\varphi > 0$ the hazard increases over time. This can be used to model end of life failure. If $\varphi = 0$, the Gompertz reduces to an exponential distribution. When the Gompertz hazard is not constant, it is always increasing or decreasing over time. This makes it particularly attractive when it is clear that operational time is the greatest force of mortality.

The Weibull distribution also introduces an exponential effect in the hazard function over time. Yet, it does so in a more flexible manner.

\begin{align*}
 h(t) = \lambda \nu t^{\nu - 1} \tag{ $\lambda >0, \nu > 0$ }
\end{align*}

It introduces a shape parameter $\nu$ that also controls the increase or decrease of the hazard over time. It is more flexible than the Gompertz in that it is capable of modeling hazards that increase initially, but whose rate of increase declines over time when $(0 < \nu < 1)$. The Weibull also has a common alternative parameterization, which is common in many software packages.

\begin{align*}
h(t) = \frac{\alpha}{\sigma} \left (\frac{t}{\sigma}  \right )^{\alpha - 1} \tag{$\alpha > 0, \sigma > 0$}
\end{align*}

This parameterization defines a shape, $\alpha$, and a scale, $\sigma$, parameter. These parameters are equivalent to the rate, $\lambda$ and shape, $\nu$, in the earlier form such that, $\lambda = \sigma^{-\alpha}$ and $\nu = \alpha$.


The Weibull distribution was originally developed to assess the catastrophic failure of materials. No material is perfectly uniform and all contain irregularities at some scale. These irregularities such things as pores, mineral inclusions or micro-cracks are distributed throughout the material. While under pressure any one of these irregularities can induce failure. As a result, the Weibull formalizes the notion of the "weakest link"\cite{Rinne2008}. It is the minimum of any collection of independent identically distributed random variables. This is important in the reliability of complex systems as different causes of system failure compete with one another and the first cause will result in the failure of the entire system\cite{Quinn2010}.

Gamma distribution, like those before it, is also a generalization of the exponential. However, its justification for use as a lifetime distribution is a bit more involved. A system may be exposed to a number of shocks, each of which are exponentially distributed. The system may be resilient to each shock up to a threshold, upon which it fails. The sum of each of those exponentially distributed shocks is gamma distributed\cite{Tso2010}. The hazard function is as follows:

\begin{align*}
h(t) = \frac{\lambda^k t^{k-1} \exp(-\lambda t)}{(1 - I_k(\lambda t)) \Gamma(k)} \tag{$ k>0, \lambda > 0 $}
\end{align*}

With $k$ being the number of exponentially distributed shocks that occur prior to system failure. As can be seen the hazard is quite complex, making use of both the gamma function $\Gamma(k)$ and the incomplete gamma integral $I_k(\lambda t)$.

$$ \Gamma(k) = \int_0^\infty x^{k-1} \exp(-x) dx \qquad I_k(x) = \frac{\int_0^x s^{k-1} \exp(-s) ds}{\Gamma(k)} $$

In traditional model fitting techniques, the incomplete gamma function imposes numerical problems for parameter estimation\cite{Wienke2010}. This has resulted in the gamma function not finding widespread application in the modeling of lifetimes. Recently there has been a resurgence in the use of the Gamma distribution, especially in the Bayesian context where optimization problems are less of a hurdle to application. Additionally, hierarchical modeling makes use of the Gamma distribution as method of modeling unobserved heterogeneity among groups of lifetimes, a subject discussed further in a later section.


Other lifetime distributions exist. However, the preceding cover the overwhelming majority of common parametric distributions used to model lifetimes. Further, while all the distributions described have mathematical and contextual justifications for their use in specific contexts, these are not always of utmost importance. Historically, the fact that a distribution provides an accurate fit for the data has been an overriding justification in many applications\cite{Marshall2007}. Furthermore, it should be stressed, regardless of which distribution is selected to model lifetimes, its adequacy should be checked. A topic which will be returned to in the following chapter.


\section*{Truncation and Censoring}
\addcontentsline{toc}{section}{Truncation and Censoring}


Truncation and censoring are defining elements of time-to-event models. The functions that have been described so far have assumed an environment where the waiting time until an event is fully observed. In most real-world applications, however, it is impractical to wait until all systems in the population fail. Therefore, time-to-event models usually exist in incomplete information. Truncation and censoring provide mechanisms to formalize how that state of limited information affects the model. The following section describes how truncation and censorship affect data, provide several standardized schemes for censorship and how the phenomena can be expressed mathematically.

Truncation refers to when values outside a particular bounds are entirely omitted. Simply put, in a truncated data set, systems are only observed if they have survived up to a particular point in time. Truncation is less common in reliability contexts but does still occur. For example, if systems' lifetimes were recorded only if they had failed after a particular date. Then those systems that had failed prior to that particular date would not be recorded and thus truncated\cite{Hong2009}.

Censoring refers to when values are only known to occur within a particular range. In time-to-event analysis it is common to examine events that occur in the future. As a result, the exact timing of an event may not be known. Yet, its the direction from the observed value, is known. This observation still provides partial information about the timing of an event but not as much as a fully observed event. 

The difference between censorship and truncation can be viewed as the difference between known-unknowns and unknown-unknowns. If truncation exists in a data set, observations are omitted from that data set and it is impossible to determine if those observations included events or not. With censoring, the source of the observations is retained, but there is uncertainty as to the exact timing of the observation. Both have detrimental effects on model fit, but the effect of truncation results in substantially greater bias.

There are different types of censoring depending on which segment of the data is missing. If the event in question is missing because it is in the future, right-censoring is said to present. If an event has already occurred but the object was not observed at the time, left-censoring is said to present. Interval censoring occurs when an event is known to have occurred within a specific range of times, but its exact timing is not known. 

Censoring is also categorized depending on the mechanism leading to observations being censored. Different schemes exist, mainly from the literature on clinical trials, which define mechanisms where censorship occurs. This includes, if the event occurs after a particular end date (Type I) or if it occurs after a specific number of events have been observed (Type II). However, more common, is random or non-informative, censoring. As the name suggests, this is when censoring occurs randomly and is independent of failure times.

Censoring can be operationalized by augmenting the underlying distribution of the time until our well-defined event, $T$. First, a binary random variable $d$ is defined. This variable takes on a value depending on what is observed at time $t$. If at time $t$ the event has occurred, then $d$ takes on the value of one. If at time $t$ the event has not yet occurred and is therefore censored, $d$ takes on the value of zero. With the introduction of $d$ it is clear that $T$ is not being directly observed. Rather, another distribution which contains censoring times $C$ is interfering. What is being observed is instead the minimum of both the failure times and the censoring times, which we denote as $Y$.

$$  Y_i = \min(T_i,C_i) \qquad \text{ and } \qquad d_i = \left\{\begin{matrix} 1 \text{ if } T_i \le C_i\\ 0 \text{ if } C_i < T_i \end{matrix}\right. $$

This provides a route to expressing what is actually being observed within a time-to-event data set, which is a combination of two distributions, one of real lifetimes, the other of censoring. 



\section*{Hazard Modeling with Covariates}
\addcontentsline{toc}{section}{Hazard Modeling with Covariates}


Hazards provide the means by which to assess the risk status of any particular system in the population over any given interval. This makes it an ideal metric through which to model the probability of failure. The constructions discussed so far have had an implicit assumption. Namely, that the lifetime distribution $T$ arises from systems under identical conditions. Effectively, this implies that the population from which these lifetimes are derived is homogeneous. In practice, it is obvious that not all systems will be subject to the identical risks and conditions. Systems will have distinct manufacturers and locations, not to mention being exposed to an assortment of risks due to different operating conditions. However, to enable this diversification of lifetimes, a method to integrate covariates that formalizes this diversity is needed. The following section describes the Multiplicative Hazard Model which provides a means to express this diversity, and demonstrates several extensions to the model to allow for more complex covariates.

To incorporate covariates into the model of lifetimes, a function, $g(\cdot)$, that permits offsets for each individual system, $i$, is required\cite{Cleves2008}\cite{Tableman2004}.

$$ h_i(t|\textbf{x}_i) = g(t, \beta_1 x_1 + \dots + \beta_m x_m)  $$


This function should alter the hazard by a given vector of covariates, $\textbf{x}_i = (x_1, \dots, x_n)$, for each individual system, and a shared vector of coefficients $\boldsymbol\beta = (\beta_1, \dots, \beta_m)$. Together, this is a linear model, $\boldsymbol\beta^T \textbf{x}_i = \beta_1 x_1 + \dots + \beta_m x_n$.

There are numerous routes to finding the function $g(\cdot)$. Yet, it is important to keep in mind the constraint on the original hazard. Any hazard must be positive, this means that the output of $g(\cdot)$ must also be positive. There is nothing to prevent the linear model from producing a negative output. However, a simple logarithmic transformation can prevent this outcome.

\begin{align*}
\ln(Y) = \boldsymbol\beta^T \textbf{x}_i \\
Y = \exp(\boldsymbol\beta^T \textbf{x}_i)
\end{align*}

Once the positive output hazard is ensured, there remains the question of the functional form of $g(\cdot)$. The most common solution is to treat the linear model as acting multiplicatively on a shared baseline hazard, $h_0(t)$.

$$ h_i(t|\textbf{x}_i) = h_0(t) \exp(\boldsymbol\beta^T \textbf{x}_i) $$

The baseline hazard, $h_0(t)$ is the common hazard when all covariates are zero, as $\exp(0) = 1$. In this way, it is the considered the baseline as it occurs prior to considering any additional variables\cite{Kleinbaum2005}. Note, the linear predictors do not contain an intercept term and rather begin with $beta_1 x_1$. This is because the baseline hazard already acts as an intercept for the model. The absence of the effect of the covariates would simply be the baseline hazard at that time.

This is the simplest form of the Multiplicative Hazard Model\footnote{Also known as the Cox Regression Model}. Whereas in a standard linear model, the estimated parameter value implies a linear change in every individual's fitted value, in this Multiplicative Hazard Model, a change in an estimated parameter implies a proportional shift in each individuals hazard at all time points\cite{Mason2015}.

There is nothing deterministic about the use of multiplication as the form of $g(\cdot)$. The linear model could have just as easily been added rather than multiplied. There exists an entire class of additive hazard models\cite{Lin1997}. However, they are far less common in practice, mainly due to difficulties fitting these models\cite{Boshuizen2010}. This lack of application has led to a smaller quantity of available literature.

This multiplicative construction separates the baseline hazard, $h_0(t)$ from the linear model, $\boldsymbol\beta^T \textbf{x}_i$. This allows for the baseline hazard and linear model to be estimated separately of each other. This greatly simplifies the fitting process. Furthermore, it allows for the comparison of the effect of any set of covariates through a proportion, known as a hazard ratio.

$$ \frac{h(t|x_1)}{h(t|x_2)} = \frac{h_0(t) \exp(\boldsymbol\beta^T x_1)}{h_0(t) \exp(\boldsymbol\beta^T x_2)}  = \exp( \boldsymbol\beta^T(x_1 - x_2) )$$

In the above ratio, it is clear that time, $t$, along with the baseline hazard, $h_0(t)$, are factored out of the final construction. In this form, the hazard is not dependent on time, just on the effect of the covariates. This specific form of the Multiplicative Hazard Model is known as the Proportional Hazards Model.

The feature that allows for time to be factored out of the model is also a limitation. In separating the baseline hazard from the linear model, a hard assumption is made about the nature of the covariates. Specifically, that they must be time-invariant, or remain static through time. This does apply to many types of covariates that can specify the level of risk to which a system is exposed, like its location, manufacturer or whether it is part of a control group. 

Fortunately, feature engineering methods can be used to construct variables that are time-invariant from ones that are time-dependent in raw data. This allows for any variables to be used within these models. Furthermore, it is possible to extend the Multiplicative Hazard model to explicitly include both time-dependent effects and time-varying covariate\cite{Dekker2008}. However, due to the complexity of the model fitting mechanisms of these constructions, especially with Frailties, these are beyond the scope of this text. 


\section*{Frailty}
\addcontentsline{toc}{section}{Frailty}


So far, all the models described have implicitly assumed that given a set of covariates, the resulting lifetimes are independent and drawn from the same underlying distribution with the same parameters. In practice, such an assumption, while useful, is rarely realistic. Thus, there is a need to extend the model further to allow for correlation between observations. This is done through the introduction of a Frailty into the model.

A Frailty\footnote{Also known as a mixed/random effect, longevity factor, susceptibility or liability. When introduced into the model, it makes a mixed hazard model, or a hierarchical hazard model} is an unmeasured random effect that is incorporated into a hazard function to account for heterogeneity in the population\cite{Hosmer2008}. This unobserved random quantity impacts the hazard function multiplicatively.

$$ Z\cdot h(t) $$

At its core, the Frailty provides a means by which to account for the fact that data often contains groupings or clusters of similar observations even if not explicitly modeled, leading to dependence among observations. This dependence implies some unknown correlation structure within and between these groups. The inclusion of a Frailty provides a method to model this dependence. It assumes conditional independence among lifetimes given the Frailty. This shared Frailty is the source of dependence within a group\cite{Wienke2010}. For example, inverters clustered within solar parks will exhibit similar lifetimes as a result of being exposed to similar conditions. Furthermore, inverters of a similar type or manufacturer are also likely to share attributes that lead to correlations in their lifetimes. This provides valuable information about the health of units within these groupings allowing for more accurate prediction of their lifetimes.

Yet, much of these similar conditions are unlikely to be fully captured by the covariates in the model. This omission occurs when relevant covariates cannot be observed or are too costly to observe. It is impossible to collect all the risk factors that contribute to failure. There is rarely awareness of all contributing factors, either because their relevance is not known or because there is a lack of means to measure them. This is especially the case for complex electronic systems whose components interact with one another as well as the environment. The cost of observation is of particular relevance to the photovoltaic domain, as the inclusion of relevant data is limited by financial constraints.

The form of the frailty determines how correlation structure is modeled. A Frailty is introduced into the Multiplicative Hazard Model as follows:

$$ h_i(t|\textbf{x}_i, Z_i) = Z_i \cdot h_0(t) \exp(\boldsymbol\beta^T \textbf{x}_i)  $$

Here, $Z$, is defined as a non-negative random variable, with some distribution, $\mathfrak{g}(\cdot)$, varying across the population\cite{Wienke2010} with $Z_i$ being the realization of the Frailty for each individual system. Generally, the Frailty distribution is standardized, resulting in an average value equal to one, $\mathrm{E}[Z] = 1$, and a variance parameter that is estimated from the data, $\text{Var}(Z) = \theta$.

It is important to distinguish between the frailty itself and the random effect, $b_i^t$:  

\begin{align*}
h_i(t|\textbf{x}_i, Z_{i}) = h_0(t) \exp(\boldsymbol\beta^T \textbf{x}_i + b_i^t z_i)
\end{align*}

Where $Z_{i} = \exp(b_i^t z_{i})$ and clearly when $b_i = 0$ then $Z_i = 1$. This illustrates that the multiplicative effect the Frailty intends to model can alternatively be viewed as an additional covariate in the linear model. 


As the average value of the Frailty is set to one, divergences from the average characterize the effect on the hazard. Individual systems with Frailties greater than one will have a higher than average hazard, implying they are more frail. Meanwhile, individual systems with Frailties less than one will have lower than average hazard implying they are less frail. Those that are more frail will die earlier than those that are less frail.

Estimating a regression model without the introduction of random effects can bias the resulting predictions if individual observations are not truly independent. In time-to-event data, this issue is compounded by temporal effects. Individual systems that are more robust will almost certainly live longer and produce more data. The population hazard declines as a result of high-risk individuals failing, but the individual hazard may continue to increase. This systematic observation of the most robust individuals will skew the computation of the average hazard. Thus, an estimate of the individual hazard rate without taking into account unobserved Frailty will underestimate the hazard function to an increasingly greater extent as time goes by\cite{Aalen2008}.

The estimation of the variance parameter, $\theta$, describes the degree of diversity in baseline hazard of a population. A large $\theta$ implies a high degree of heterogeneity as lifetimes deviate heavily from the average value of one. Conversely, a small $\theta$ implies the homogeneity of lifetimes as each one will be heavily concentrated near the average value.


In the above construction, the Frailty is introduced in such a way that the entire population of systems is covered by a single distribution. In practice, it is more useful to estimate a Frailty for each distinct group within the population, as follows:

$$ h_{ij}(t|\textbf{x}_i, Z_j) = Z_j \cdot h_0(t) \exp(\boldsymbol\beta^T \textbf{x}_{ij})  $$

Here, a Frailty of a collection is introduced and each distinct group $j$ is composed of a total of $k$ distinct groups. A random variable, represents the collection, $Z_j = (z_1, \dots, z_k)$. A collection is defined as a set of groups that all share a common characteristics. To be properly defined, a collection should be mutually exclusive and exhaustive given the data. For example, there can only be a finite number of parks within which inverters reside. Each park makes up a distinct group, and all of the parks make up the collection. With this structure, it is possible to estimate the Frailty, and thus the level of heterogeneity for each group. The interpretation of the Frailty and its variance are identical to what has been described previously, except localized to each distinct group. This can be extended further to allow for as many different collections as is required, simply by introducing additional Frailties into the model multiplicatively. It should be noted, the multiplicative introduction of further Frailties implies independence between lifetimes of different collections. In fact, the lifetimes of individual systems are conditionally independent given the vector of Frailties. 


Any distribution which is positive and possesses a mean can be set to one, can be used as the Frailty distribution, $\mathfrak{g}(\cdot)$. The most commonly used distribution is the Gamma. It is utilized due to its flexibility in modeling positive outcomes as well as the ease with which it allows for the expression of all required formulas\cite{Wienke2010}. The density of the Gamma is given as follows:

\begin{align*}
f(t) = \frac{\lambda^k t^{k-1} \exp(-\lambda t)}{\Gamma(k)} \tag{$ k>0, \lambda > 0 $}
\end{align*}


As stated earlier, the goal here is to fix the average Frailty to one and estimate the variance. This can be done the following way:

\begin{align*}
\mathrm{E}[Z] = \frac{k}{\lambda} = 1 \tag{$ k = \lambda $}
\end{align*}


\begin{align*}
\mathrm{V}[Z] = \frac{k}{\lambda^2} = \frac{1}{\sigma^2} \tag{$ k = \lambda $}
\end{align*}


The restriction of $k = \lambda$ is made to ensure that the average Frailty is equal to one. Through this restriction we note that the variance is now, by definition, $\sigma^2 = \frac{1}{\lambda}$. This leads to the conditional density of the Gamma distributed Frailty:

$$ f(z) = \frac{1}{\Gamma(\frac{1}{\sigma^2})} \left (\frac{1}{\sigma^2}  \right )^{\frac{1}{\sigma^2}} t^{\frac{1}{\sigma^2} - 1} \exp(-\frac{t}{\sigma^2}) $$

This density can be transformed into a unconditional survival function and consequently the into the unconditional density and hazard using the following formulas:

$$ f(t) = \frac{h_0(t)}{(1+ \sigma^2 H_0(t))^{\frac{1}{\sigma^2}+ 1 } }$$

$$ S(t) = (1 + \sigma^2 H_0(t))^{- \frac{1}{\sigma^2}}$$

$$ h(t) = \frac{h_0(t)}{1 + \sigma^2 H_0(t)} $$


The corresponding hazard and cumulative hazard can be replaced with the appropriate lifetime distribution. For example, let $h(t) =  \frac{\alpha}{\gamma} \left (\frac{t}{\gamma}  \right )^{\alpha - 1}$ be Weibull distributed baseline hazard with a Frailty following a Gamma distribution $Z \sim \text{Gamma}(\frac{1}{\sigma^2}, \frac{1}{\sigma^2} )$. Thus, the unconditional survival and hazard functions are given by the following expressions:

$$ S(t) = \left (1 + \sigma^2 \left (\frac{t}{\gamma}  \right )^{\alpha}  \right )^{-\frac{1}{\sigma^2}} $$

$$ h(t) = \frac{\frac{\alpha}{\gamma}\left (\frac{t}{\gamma}  \right )^{\alpha - 1}}{1 + \sigma^2 \left (\frac{t}{\gamma}  \right )^{\alpha} } $$

Where $\sigma^2$ represents the variance of the Gamma distributed Frailty.

There are several important issues that should be kept in mind when introducing Frailties to time-to-event models. A Frailty is assumed to be constant over time. If the random variable $Z$ is introduced as a Frailty into the Multiplicative Hazard Model, then the estimation of $Z$ is fixed for all time points. However, this does not mean that Frailties experience no temporal effects. This can be seen when the conditional expectation and variance of the Frailty $Z$ are derived:

$$\mathrm{E}[Z|\textbf{x},  T > t ] = \frac{1}{1 + \sigma^2 H_0(t) \exp(\boldsymbol\beta^T \textbf{x})} $$

$$ \mathrm{V}[Z|\textbf{x},  T > t ] = \frac{\sigma^2}{(1 + \sigma^2 H_0(t) \exp(\boldsymbol\beta^T \textbf{x}))^2} $$

Two things of note can be understood from the above. First, individuals dying at time $t$ will have a higher mean Frailty compared to survivors. This is because the cumulative hazard in the denominator increases as time goes on. Second, the variance of Frailty declines over time. This implies that early failures reduce the heterogeneity of of the population over time. However, the ratio between the average Frailty and its variance remain constant over time implying that the population does not become more homogeneous relative to the average in time\cite{Wienke2010}.

The introduction of a Frailty, at any level, will invalidate the Proportional Hazards property\cite{Kleinbaum2005}. The property, explained in the previous section, allows for the a comparison between any set of covariates through a simple proportion. However, with the introduction of the Frailty, any sets of covariates now require the computation of a temporal effect, such that:

$$ \frac{h_1(t)}{h_2(t)} = \frac{1 - \sigma^2 H_0(t)}{1 - \sigma^2 H_0(t) \exp(\boldsymbol\beta)} \exp(\boldsymbol\beta) $$

The ratio of specific population hazards is generally not time-invariant. This can be seen from the above equation, the hazard ratio is a decreasing function, unless there is no difference between groups or the Frailty is not relevant. The reason for this are, again, the alteration of the hazard rate in the population as high-risk individuals fail earlier leading to a lower average hazard rate over time.

There are significant advantages to using a parametric baseline hazard when introducing Frailties into a Multiplicative Hazards Model. The most important consequence of such a combination is that it enables the explicit description of the evolution of a Frailty over time. Without a parametric form it is difficult to describe how 

In a traditional context, the values of $Z_i$ are not estimated for each individual system within the population. The reasoning for this is relatively simple. An estimate of a $Z_i$ for each individual system, $i$, would require $N$ parameters. This would result in more parameters in the model than there are observations, leading to an over-saturated model. Furthermore, the estimation of individual Frailties are usually less important than group Frailties and variances. These provide useful estimates of the degree to which these groupings affect survival times, as well as identify patterns among those groups. That said, this traditional restriction is loosened in the context of Bayesian estimation where an estimation for $Z_i$ can be sampled.




\section*{Model Likelihoods}
\addcontentsline{toc}{section}{Model Likelihoods}


The likelihood is a function of the parameters of a statistical model given data. Its formulation is the means by which the appropriate mathematical abstraction is selected after data has been observed. In this section, likelihoods for the models developed so far are formulated. First in the simplest cases, then with the model additions.

Generally, a parametric model is fit by finding the maximum values for a set of parameters, $\theta$, of its probability density function, $f(\cdot)$, given a vector of data, $T = (t_1, \dots, t_n)$. This process is encapsulated in Maximum Likelihood Estimation (MLE):

\begin{align*}
\displaystyle \arg \max _{\theta }{\mathcal {L}}(\theta | T) &= \arg \max _{\theta }f(T|\theta ) \\
{\mathcal {L}}(\theta | T) = \Pr(T | \theta) &= \prod_i f_{\theta}(t_i)
\end{align*}

In the time-to-event context, there is an added complication. As noted earlier, this type of data often features censoring and truncation. When non-informative right-censoring is present, it is required to extend the likelihood. The data, $T$, is now input as a pair, $(y_i, d_i)$. The $Y$ random variable is defined as the minimum of the actual and censored lifetime. It represents the observed lifetimes found in the data. A binary random variable, $d$, is introduced to make explicit when censoring occurs. The likelihood is altered as follows:

$$ \mathcal {L}(\theta) = \prod^n_i f(y_i)^{(d_i)} S(y_i)^{1 - d_i} $$

The probability density function, $f(y_i)$, of the parametric distribution is still present in the above equation, much like in the standard likelihood. However, its contribution is now controlled by the $d$ random variable. When the variable $d$ equals one, it implies a failure event and the likelihood is evaluated in the general way. However, when $d$ is zero,  censoring occurs and the likelihood evaluates instead the survival function, $S(y_i)$, which implies the object has survived up to that particular point in time. The combination provides information about both observed events and about yet to be observed events. 

Given the identities found at the beginning of this chapter, it is possible to transform the joint likelihood to make use of hazards.  Such that the likelihood becomes:

$$ \mathcal {L}(\theta) = \prod^n_i h(y_i)^{d_i} \exp(-H(y_i)) $$

This generally provides easier functions to work with when constructing the likelihood. It should be noted that hazard estimation is restricted here to only include parametric forms. It is incredibly common to not make parametric assumptions about the baseline hazard. There is a large body of research into non-parametric baseline hazard estimation, including the Kaplan-Meier and Nelson-Aalen estimators. If contrasting the effect of covariates is the predominant purpose of the model, semi-parametric or non-parametric methods are often preferred. However, the structure of the baseline hazard has substantive implications for the ability to understand the model and predict. This is especially the case when Frailties are involved as parametric hazards aide in understanding the evolution of group hazards over time. Furthermore, non-parametric methods lack the ability to forecast future hazard rates beyond the last failure time, as any value beyond the last time is simply undefined. That said, the parametric form is not without its price, namely it is an assumption about the evolution of lifetimes which must be verified to be used.

The likelihood for the censored model can be extended to include covariates. First, recall the Multiplicative Hazards Model:

$$ h_i(y|\textbf{x}_i) = h_0(y) \exp(\boldsymbol\beta^T \textbf{x}_i) $$

In this case the data is input as a triple, $(y_i, d_i, \textbf{x}_i)$ as the covariates are also introduced. The likelihood for this function is as follows:

$$ \mathcal {L}(\theta) = \prod^n_i \left (h_0(y_i) \exp(\boldsymbol\beta^T \textbf{x}_i)  \right )^{d_i} \exp(-H_0(y_i) \exp(\boldsymbol\beta^T \textbf{x}_i)) $$

The introduction of a univariate Frailty into the Multiplicative Hazard Model is straight-forward. As stated earlier, the Frailty model can be generally stated as:

$$ h_{i}(y|\textbf{x}_i, Z) = Z_i \cdot h_0(y) \exp(\boldsymbol\beta^T \textbf{x}_{i})  $$

Thus, the likelihood with univariate Frailty and non-informative right censoring simply expands to:

$$ \mathcal {L}(\theta) = \prod^n_i \left (Z_i \cdot h_0(y_i) \exp(\boldsymbol\beta^T \textbf{x}_i)  \right )^{d_i} \exp(-Z_i \cdot H_0(y_i) \exp(\boldsymbol\beta^T \textbf{x}_i)) $$

As can be seen from the above, a product is now required to handle the additional, $k$, Frailty terms. In each extension the effect is simply the multiplication of an additional term, or set of terms, on the baseline hazard.

If a Gamma Frailty is used, and the parameters, $Z$ are integrated out, the likelihood of the univariate Frailty becomes: 

$$ \mathcal {L}(\theta) = \prod_{i=1}^n \left (\frac{h_0(y) \exp(\boldsymbol\beta^T \textbf{x}_i)}{1+ \sigma^2 H_0(y) \exp(\boldsymbol\beta^T \textbf{x}_i)}  \right )^{d_i} (1+ \sigma^2 H_0(y) \exp(\boldsymbol\beta^T \textbf{x}_i))^{-\frac{1}{\sigma^2}} $$

The extension to shared Frailty adds an additional complication. Specifically, for each cluster a separate frailty distribution must be fit. As right censoring is at play, this vastly complicates the model likelihood. For, $n$, clusters, $j$, of size, $n_i$,, such that, $i \dots, n$, the shared frailty becomes\cite{Wienke2010}:

$$ \mathcal {L}(\theta) =\prod_{i=1}^n \frac{\Gamma(\frac{1}{\sigma^2} + \delta_i) \prod^{n_i}_{j=1} (h_0(t) \exp(\boldsymbol\beta^T \textbf{x}_{ij}))^{d_{ij}} }{(\frac{1}{\sigma^2} + \sum_{j =1}^{n_i} H_0(t) \exp(\boldsymbol\beta^T \textbf{x}_{ij}))^{\frac{1}{\sigma^2} + \delta_i} \sigma^{\frac{2}{\sigma^2}} \Gamma(\frac{1}{\sigma^2})} $$

Where $\delta_i = \sum^{n_i}_{j=1} d_{ij}$ or the count of events within each cluster and the Gamma function is $\Gamma (t) = \int _{0}^{\infty }x^{t-1}\exp(-x)dx$. Taking the log of the above, we get the log likelihood\cite{Duchateau2008}:

\begin{align*}
\ell(\theta)& = \sum^n_{i=1}  \Bigg[\delta_i \ln(\sigma^2) + \ln \Gamma\left (\frac{1}{\sigma^2} + \delta_i  \right )  - \ln\Gamma\left (\frac{1}{\sigma^2}  \right )\\
& - \left (\frac{1}{\sigma^2} + \delta_i  \right ) \ln \left (1 + \sigma^2 \sum_{j=1}^{n_i} H_0(t)\exp(\boldsymbol\beta^T \textbf{x}_{ij})  \right )\\
& + \sum_{j=1}^{n_i} d_{ij} \left (\boldsymbol\beta^T \textbf{x}_{ij} + \ln h_0(t) \right )  \Bigg]
\end{align*}

% As we can see this likelihood is incredibly complex. Yet, the Gamma Frailty is one of the few that is capable of being fit in this manner, as it has a closed form solution for the log likelihood. If 





There are many technical details which are required to properly fit a Frailty model. Most of these are specialized and are far beyond the scope of this text. The reader is directed to the "Extended Reading" section at the end of this chapter for more details on this front.

It should be noted, that these are intentionally generic and the parametric forms for both the baseline hazard and the Frailty are required to properly estimate the model.




\section*{Model Estimation}
\addcontentsline{toc}{section}{Model Estimation}


Model estimation can be done in a number of ways. However, as model complexity increases with the addition of covariates, random effects or other features, increasing difficulties in using traditional methods of model fitting are encountered. Many of these issues arise from a lack of closed form solutions for the required mathematical constructions needed to fit these models. Bayesian methods provide a means to circumvent some of this difficulty through the use of sampling rather than explicit formulation to derive the model output. In this section we examine the Bayesian paradigm and provide some intuition as to the model fitting technique used in the remainder of the text, Hamiltonian Monte Carlo (HMC).

In the preceding section, likelihoods for the various models were presented. It was stated that these parametric models could be fit by finding the maximum of the likelihood function. While in simple cases this is practical, as model complexity increases this becomes considerably more difficult. The introduction of random effects, like shared Frailties, pose an especially large challenge as closed form solutions for the models either become too complicated or cease to exist. In the case of Gamma Frailties, these closed form solutions still exist but are remarkably complex. The final equation from the previous section is an affirmation of that fact. However, if the model were to be extended further to introduce a different parametric distribution for the shared Frailty, the likelihood would lack an explicit formulation. For example, log-normal Frailties no longer have a closed form solution for a likelihood that factors out the Frailty. In these cases, Expectation-Maximization, Adaptive Gaussian quadrature or Markov Chain Monte Carlo (MCMC) are some of the few available numerical optimization methods that can be employed\cite{Wienke2010}. While the exact process for fitting such model extensions is beyond the scope of this text, it is important to provide the route by which the model could be extended further. For that, a flexible fitting mechanism is required. In this case, Hamiltonian Monte Carlo, which is a special case of MCMC is used. 

In the Bayesian paradigm, the posterior distribution encompasses the credibility of the parameter values, of a particular model. Unlike in traditional statistics, the vector of unknown parameter values, $\theta$, is assumed to be random variable with its own distribution as well as a prior distribution, $\Pr(\theta)$\cite{Ibrahim2005}. Inferences concerning the parameter values are based on the posterior distribution, given by:

$$ \Pr(\theta|D) = \frac{\mathcal {L}(\theta|D)\Pr(\theta)}{\int_\theta\mathcal {L}(\theta|D)\Pr(\theta) d\theta} $$

Where $D$ is the observed data. It can be seen from the above equation, that the posterior probability is proportional to the likelihood multiplied by the prior. 

$$ \Pr(\theta|D) \propto  \mathcal {L}(\theta|D)\Pr(\theta) $$

This is because the denominator is a normalizing constant: 

$$ \int_\theta\mathcal {L}(\theta|D)\Pr(\theta) d\theta $$

This normalizing constant is interesting because in Bayesian models, it often lacks a closed form solution and has to be estimated through an alternative process. This produces a similar situation to the one found in the time-to-event models. When the closed form solution is not available, similar methods as those used for fitting Bayesian models become useful. However, even when closed form solutions are available, the Bayesian method of fitting may prevent the need to manually integrate a complex likelihood. Additionally, features like shared Frailties can be re-contextualized as Bayesian hierarchical models. This forestalls the requirement for asymptotic arguments required to compute model extensions. In contrast, in the Bayesian context, these features are simply by-products of sampling from the posterior and their calculation becomes substantially simpler\cite{Ibrahim2005}. 

The most common approach for describing a posterior distribution make use of MCMC methods. These methods generate representative samples from the posterior distribution. The earliest method for generating representative samples, also known as the Metropolis-Hastings algorithm\cite{Metropolis1953}, proceeds as follows: First, an arbitrary starting point along the distribution is selected and its value is recorded. This value is the posterior probability at that point. Next, a candidate point is randomly selected around the starting point. If the value of the candidate point is greater than the starting point, then the algorithm moves to that new point. If the value of the candidate point is less than the starting point, then the the move is made probabilistically. The probability of the move is determined by the ratio of values between the starting point and the candidate point. Such that:

$$ \Pr_{\text{accept}} = \min\left (\frac{\Pr(\theta_{\text{proposed}}|D)}{\Pr(\theta_{\text{current}}|D)} , 1 \right )$$

Once the algorithm decides whether or not to move, the process begins again, with that position being the new starting point. This process is repeated a very large number of times, which creates a representative sample of the posterior distribution.

There have been numerous attempts to improve the efficiency of this simple algorithm. The largest inefficiencies arise in two areas. The tails of the distribution require a large amount of iterations to accurately map. This is because each step toward those tails has a very small probability of being accepted. For much the same reason, distributions with several local optima require a greater amount of iterations for the algorithm to escape these optima. Many of the attempts to improve the efficiency of this algorithm have tried to address these deficiencies. The most recent is Hamiltonian Monte Carlo (HMC), which was codified into the Stan statistical software package\cite{Carpenter2016}. 

The central difference between the Metropolis-Hastings algorithm and HMC, is the mechanism for determining the proposal distribution\cite{Kruschke2015}. 

After the algorithm selects a starting point, it has to randomly select a candidate point. These candidates are usually drawn from a symmetric distribution like a multivariate Gaussian. This fixed shape leads to candidates being selected regardless of where in the distribution the starting point is. As a result, candidates can just as easily travel away from the posterior mode as towards it. HMC uses a proposal distribution that differs based upon the current position. It alters the proposal by calculating the gradient, or the direction of change, of the posterior distribution. It then distorts the proposal distribution to match that gradient.

The process of selecting from candidates is also altered in HMC. Rather than simply selecting a point from the proposal distribution and evaluating it, HMC uses a 'momentum', around the candidate point. The area around the selected point is then allowed to be sampled given this momentum. The final point is selected based on the value that the point 'rolls' into\cite{Kruschke2015}. The probability of acceptance takes into account this momentum when finally deciding on a move.

$$ \Pr_{\text{accept}} = \min\left (\frac{\Pr(\theta_{\text{proposed}}|D)\Pr(\phi_{\text{proposed}}|D)}{\Pr(\theta_{\text{current}}|D)\Pr(\phi_{\text{current}}|D)} , 1 \right )$$

As before, this process is repeated a multitude of times until the posterior probability distribution is approximated. From this approximation, it is possible to extract the features necessary to enable model fitting. These alterations make HMC more computationally costly, as the gradient is required to be calculated. However, it is also considerably more efficient as fewer samples are required to build a representative posterior distribution.


Numerous additional technical details are required to properly understand HMC which are not covered in this text, such as No U-Turn sampling, step size and duration tuning. Fortunately, software will optimize these particularities for a user. If the reader is interested in these topics, the 'Extended Reading' section provides some references of interest.



\section*{Extended Reading}
\addcontentsline{toc}{section}{Extended Reading}

For additional treatments of the topic, several notable sources can be accessed. For a comprehensive mathematical treatment of classical survival analysis, the reader is directed to Klein and Moeschberger\cite{Klein2003}. For a mathematical treatment of survival analysis based on counting processes, the reader is directed to Andersen\cite{Andersen1992} and Aalen, Borgan and Gjessing\cite{Aalen2008}. If the reader is interested in a pedagogical viewpoint on survival analysis or model computation, the reader should explore Kleinbaum and Klein\cite{Kleinbaum2005} and Tableman and Kim\cite{Tableman2004}. For a treatment of Bayesian time-to-event analysis including extensions to this model, the reader is directed to Ibrahim, Chen and Sinha\cite{Ibrahim2005}. For a detailed description of Frailty Modeling, including detailed mathematical derivations of incrementally complex models, Wienke\cite{Wienke2010}, as well as Duchateau and Janssen\cite{Duchateau2008} should be explored. For more details on Hamiltonian Monte Carlo, the reader is directed to Kruschke\cite{Kruschke2015}, Gelman et al\cite{Gelman2014}. For a technical review of Bayesian estimation, Marin and Roberts\cite{Marin2007} are suggested.


